{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vad.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHwfyt+5WKPBum0pUgEjni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimregan/wav2vec2-sprint/blob/vad/vad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo0Hy3GcHQit"
      },
      "source": [
        "!pip install librosa webrtcvad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRGZSK6eHTLq"
      },
      "source": [
        "Taken from [PyTorch Speaker Verification](https://github.com/HarryVolek/PyTorch_Speaker_Verification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux8SX2YrHoCh"
      },
      "source": [
        "# Licence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRyoNkF0HdWh"
      },
      "source": [
        "BSD 3-Clause License\n",
        "\n",
        "Copyright (c) 2019, HarryVolek\n",
        "\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice, this\n",
        "  list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice,\n",
        "  this list of conditions and the following disclaimer in the documentation\n",
        "  and/or other materials provided with the distribution.\n",
        "\n",
        "* Neither the name of the copyright holder nor the names of its\n",
        "  contributors may be used to endorse or promote products derived from\n",
        "  this software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwZjkeQzHzOC"
      },
      "source": [
        "# VAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5f0-XyBH2Gz"
      },
      "source": [
        "\"\"\"\n",
        "Created on Tue Dec 18 16:22:41 2018\n",
        "\n",
        "@author: Harry\n",
        "Modified from https://github.com/wiseman/py-webrtcvad/blob/master/example.py\n",
        "\"\"\"\n",
        "\n",
        "import collections\n",
        "import contextlib\n",
        "import numpy as np\n",
        "import sys\n",
        "import librosa\n",
        "import wave\n",
        "\n",
        "import webrtcvad\n",
        "\n",
        "#from hparam import hparam as hp\n",
        "sr = 16000\n",
        "\n",
        "def read_wave(path, sr):\n",
        "    \"\"\"Reads a .wav file.\n",
        "    Takes the path, and returns (PCM audio data, sample rate).\n",
        "    Assumes sample width == 2\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        assert num_channels == 1\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "    data, _ = librosa.load(path, sr)\n",
        "    assert len(data.shape) == 1\n",
        "    assert sr in (8000, 16000, 32000, 48000)\n",
        "    return data, pcm_data\n",
        "    \n",
        "class Frame(object):\n",
        "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    \"\"\"Generates audio frames from PCM audio data.\n",
        "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
        "    the sample rate.\n",
        "    Yields Frames of the requested duration.\n",
        "    \"\"\"\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms,\n",
        "                  padding_duration_ms, vad, frames):\n",
        "    \"\"\"Filters out non-voiced audio frames.\n",
        "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
        "    the voiced audio.\n",
        "    Uses a padded, sliding window algorithm over the audio frames.\n",
        "    When more than 90% of the frames in the window are voiced (as\n",
        "    reported by the VAD), the collector triggers and begins yielding\n",
        "    audio frames. Then the collector waits until 90% of the frames in\n",
        "    the window are unvoiced to detrigger.\n",
        "    The window is padded at the front and back to provide a small\n",
        "    amount of silence or the beginnings/endings of speech around the\n",
        "    voiced frames.\n",
        "    Arguments:\n",
        "    sample_rate - The audio sample rate, in Hz.\n",
        "    frame_duration_ms - The frame duration in milliseconds.\n",
        "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
        "    vad - An instance of webrtcvad.Vad.\n",
        "    frames - a source of audio frames (sequence or generator).\n",
        "    Returns: A generator that yields PCM audio data.\n",
        "    \"\"\"\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    # We use a deque for our sliding window/ring buffer.\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
        "    # NOTTRIGGERED state.\n",
        "    triggered = False\n",
        "\n",
        "    voiced_frames = []\n",
        "    for frame in frames:\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
        "            # the ring buffer are voiced frames, then enter the\n",
        "            # TRIGGERED state.\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                start = ring_buffer[0][0].timestamp\n",
        "                # We want to yield all the audio we see from now until\n",
        "                # we are NOTTRIGGERED, but we have to start with the\n",
        "                # audio that's already in the ring buffer.\n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            # We're in the TRIGGERED state, so collect the audio data\n",
        "            # and add it to the ring buffer.\n",
        "            voiced_frames.append(frame)\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            # If more than 90% of the frames in the ring buffer are\n",
        "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
        "            # audio we've collected.\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = False\n",
        "                yield (start, frame.timestamp + frame.duration)\n",
        "                ring_buffer.clear()\n",
        "                voiced_frames = []\n",
        "    # If we have any leftover voiced audio when we run out of input,\n",
        "    # yield it.\n",
        "    if voiced_frames:\n",
        "        yield (start, frame.timestamp + frame.duration)\n",
        "\n",
        "\n",
        "def VAD_chunk(aggressiveness, path):\n",
        "    audio, byte_audio = read_wave(path, sr)\n",
        "    vad = webrtcvad.Vad(int(aggressiveness))\n",
        "    frames = frame_generator(20, byte_audio, sr)\n",
        "    frames = list(frames)\n",
        "    times = vad_collector(sr, 20, 200, vad, frames)\n",
        "    speech_times = []\n",
        "    speech_segs = []\n",
        "    for i, time in enumerate(times):\n",
        "        start = np.round(time[0],decimals=2)\n",
        "        end = np.round(time[1],decimals=2)\n",
        "        j = start\n",
        "        while j + .4 < end:\n",
        "            end_j = np.round(j+.4,decimals=2)\n",
        "            speech_times.append((j, end_j))\n",
        "            speech_segs.append(audio[int(j*sr):int(end_j*sr)])\n",
        "            j = end_j\n",
        "        else:\n",
        "            speech_times.append((j, end))\n",
        "            speech_segs.append(audio[int(j*sr):int(end*sr)])\n",
        "    return speech_times, speech_segs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjmykjXyIMPV"
      },
      "source": [
        "(From ```dvector_create.py``` in the same package)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myog5EDPH6C_"
      },
      "source": [
        "import numpy as np\n",
        "def concat_segs(times, segs):\n",
        "    #Concatenate continuous voiced segments\n",
        "    concat_seg = []\n",
        "    seg_concat = segs[0]\n",
        "    for i in range(0, len(times)-1):\n",
        "        if times[i][1] == times[i+1][0]:\n",
        "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
        "        else:\n",
        "            concat_seg.append(seg_concat)\n",
        "            seg_concat = segs[i+1]\n",
        "    else:\n",
        "        concat_seg.append(seg_concat)\n",
        "    return concat_seg"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7gdrly0IpRR"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWLCFV4dIwdi"
      },
      "source": [
        "!pip install youtube-dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FcsjkvxI91-",
        "outputId": "1be84edb-a605-43f5-db49-f1726033d24f"
      },
      "source": [
        "!youtube-dl --all-subs -o '%(id)s' VRg-a0qSGa8"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[youtube] VRg-a0qSGa8: Downloading webpage\n",
            "[youtube] VRg-a0qSGa8: Downloading MPD manifest\n",
            "[info] Writing video subtitles to: VRg-a0qSGa8.en.vtt\n",
            "\u001b[0;33mWARNING:\u001b[0m Requested formats are incompatible for merge and will be merged into mkv.\n",
            "[dashsegments] Total fragments: 88\n",
            "[download] Destination: VRg-a0qSGa8.f248\n",
            "\u001b[K[download] 100% of 57.71MiB in 00:36\n",
            "[download] Destination: VRg-a0qSGa8.f140\n",
            "\u001b[K[download] 100% of 6.86MiB in 00:00\n",
            "[ffmpeg] Merging formats into \"VRg-a0qSGa8.mkv\"\n",
            "Deleting original file VRg-a0qSGa8.f248 (pass -k to keep)\n",
            "Deleting original file VRg-a0qSGa8.f140 (pass -k to keep)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGeU9bqOKxfd"
      },
      "source": [
        "!ffmpeg -i VRg-a0qSGa8.mkv -acodec pcm_s16le -ac 1 -ar 16000 VRg-a0qSGa8.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhShQO9fLBo4"
      },
      "source": [
        "times, segs = VAD_chunk(3, 'VRg-a0qSGa8.wav')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohIO-cG0LNPY"
      },
      "source": [
        "import numpy as np\n",
        "def concat_both(times, segs):\n",
        "    \"\"\"Concatenate continuous times and their segments\"\"\"\n",
        "    concat_seg = []\n",
        "    concat_times = []\n",
        "    seg_concat = segs[0]\n",
        "    time_concat = times[0]\n",
        "    for i in range(0, len(times)-1):\n",
        "        if time_concat[1] == times[i+1][0]:\n",
        "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
        "            time_concat = (time_concat[0], times[i+1][1])\n",
        "        else:\n",
        "            concat_seg.append(seg_concat)\n",
        "            seg_concat = segs[i+1]\n",
        "            concat_times.append(time_concat)\n",
        "            time_concat = times[i+1]\n",
        "    else:\n",
        "        concat_seg.append(seg_concat)\n",
        "        concat_times.append(time_concat)\n",
        "    return concat_times, concat_seg"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOI8L_psURXV"
      },
      "source": [
        "ntimes, nsegs = concat_both(times, segs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phPPgOi5WDoF"
      },
      "source": [
        "merged = list()\n",
        "for i in range(0, len(times)):\n",
        "  cur = dict()\n",
        "  cur['start'] = times[i][0]\n",
        "  cur['end'] = times[i][1]\n",
        "#  cur['speech'] = nsegs[i]\n",
        "  merged.append(cur)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ZufqkUX6Im"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}